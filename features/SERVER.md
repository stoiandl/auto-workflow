# Auto-Workflow Server & UI

A FastAPI-based server with a lightweight UI to observe flows in real time: runs, tasks, and logs. Flows bind to the server via the existing event bus, pushing status updates as they execute. The server aggregates, stores, persists (Postgres), and streams updates to the browser. It is deployable as a Docker container in Kubernetes or App Service.

## Goals
- Real-time visibility of flow runs and task states (pending → running → ok/err)
- Minimal setup: single `uvicorn` process, in-memory storage to start
- Opt-in “binding” from any process running flows (local/dev first)
- Simple, fast UI with server-side rendering (SSR) + small sprinkles of interactivity
- Backward-compatible with existing events/logging middleware

Non-goals (initially):
- Deep RBAC and SSO (baseline API-key first; OAuth later)
- Arbitrary long-term analytics out of the gate (introduce retention and indices first)
- Complex multi-region HA (start with single region; scale horizontally with a shared DB + pub/sub)

---

## High-Level Architecture

Components:
- Client Instrumentation (in this repo):
  - An event sink that subscribes to `flow_started`, `flow_completed`, `task_started`, `task_ok`, and `task_err` and forwards them to the server via HTTP.
  - Optional log forwarding for structured lines and task-scoped logs.
- Server (FastAPI):
  - Ingest API to accept client events.
  - Postgres-backed run store (production) with SQLAlchemy 2.0 ORM (mapper API) + Alembic migrations.
  - Broadcaster that pushes updates to connected browsers via WebSocket; SSE fallback supported.
  - SSR pages (Jinja2) for: flows catalog, recent runs, run detail (live updates), task table, and logs view.
  - Optional Redis pub/sub for cross-instance WS broadcast when scaled horizontally.
- UI:
  - Jinja2 templates + minimal JS (HTMX/Alpine optional) for paging/filtering.
  - WebSocket connects when viewing a run detail page to receive incremental updates.

Data flow:
1. Flow code emits structured events (already built-in).
2. Client sink posts events to server.
3. Server updates in-memory state and persists if configured.
4. Server broadcasts deltas to WS subscribers for that run_id.
5. Browser receives updates and re-renders parts of the page.

---

## Data Model (Production)

- Project/Namespace (optional for multi-tenancy; P1)
  - id: uuid
  - key: str (slug)
  - name: str

- Flow (catalog entry)
  - id: uuid (stable ID provided by client optional; otherwise server-generated)
  - project_id: uuid | null
  - flow_key: str (stable key from code; e.g., module.path:flow_name)
  - name: str
  - description: str | null
  - tags: jsonb[] | text[]
  - created_at: timestamptz
  - updated_at: timestamptz

- Task (catalog entry)
  - id: uuid
  - flow_id: uuid (FK)
  - task_key: str (stable key; e.g., module.path:task_name)
  - name: str
  - description: str | null
  - tags: jsonb[] | text[]
  - run_in: str (async|thread|process)
  - created_at, updated_at: timestamptz

- FlowRun
  - run_id: str
  - flow_id: uuid (FK to Flow)
  - flow_name: str (cached denormalized)
  - status: str (RUNNING, SUCCEEDED, FAILED)
  - params: jsonb
  - metadata: jsonb (env, git_sha, triggered_by, schedule_id, etc.)
  - started_at: timestamptz
  - finished_at: timestamptz | null
  - tasks_total: int | None (optional; inferred at end)

- TaskRun
  - id: str (node id, e.g., `task_name:N`)
  - run_id: str (FK to FlowRun.run_id)
  - task_id: uuid (FK to Task)
  - task_name: str (denormalized)
  - status: str (PENDING, RUNNING, SUCCEEDED, FAILED)
  - started_at: timestamptz | null
  - finished_at: timestamptz | null
  - duration_ms: float | None
  - error: str | None
  - attempt: int (for retries)
  - priority: int | null
  - tags: jsonb[] | text[]

- LogLine (task/flow logs)
  - run_id: str
  - task_node: str | null
  - ts: timestamptz
  - level: str
  - message: str
  - event: str | null
  - payload: jsonb

Indexes:
- flow(flow_key), task(task_key, flow_id)
- flow_run(run_id primary key), flow_run(flow_id, started_at desc)
- task_run(run_id, task_node), task_run(status), task_run(finished_at desc)
- log_line(run_id, ts), log_line(run_id, task_node, ts)

Migrations: Alembic managed; image contains autogenerated migration scripts; helm hook applies migrations on deploy.

Statuses:
- FlowRun: RUNNING on first `flow_started`; SUCCEEDED/FAILED on `flow_completed` (derive from tasks and/or final event).
- TaskRun: RUNNING on `task_started`; SUCCEEDED on `task_ok`; FAILED on `task_err`.

---

## APIs

Public (ingest) endpoints:
- POST `/api/v1/events`
  - Body: `{ "event": "flow_started|flow_completed|task_started|task_ok|task_err|task_log", ... }`
  - Minimal required fields:
    - Common: `ts`
    - Flow events: `flow`, `run_id`, `tasks` (on completed)
    - Task events: `task`, `node`, `run_id`, `flow`, `duration_ms` (ok/err), `error` (err)
    - Task log event (`task_log`): `run_id`, `node` (optional for flow logs), `level`, `message`, `payload` (optional)
  - Response: `{"ok": true}`

Read endpoints (UI + API):
- GET `/` → redirect to `/flows`
- GET `/flows` → SSR: flows catalog and recent runs
- GET `/runs` → SSR: recent runs
- GET `/runs/{run_id}` → SSR: run detail (summary + tasks table + live updates)
- GET `/api/v1/runs/{run_id}` → JSON detail
- GET `/api/v1/runs/{run_id}/tasks` → JSON tasks
- GET `/api/v1/flows` → JSON catalog
- GET `/api/v1/flows/{flow_key}` → JSON catalog entry + latest runs

Real-time:
- WS `/ws/runs/{run_id}`: server broadcasts events `{ kind: "task|flow|log", data: { ... } }`
- SSE `/sse/runs/{run_id}` (fallback): text/event-stream

---

## Client Binding (Flow → Server)

Two options:
1) Environment variable opt-in (zero code changes):
```bash
export AUTO_WORKFLOW_SERVER_URL=http://localhost:8000
export AUTO_WORKFLOW_SERVER_API_KEY=change-me
```
At import, a small client module checks this env var and, if present, `subscribe()`s to events and posts them to `/api/events`.

2) Programmatic binding:
```python
from auto_workflow.events import subscribe
from auto_workflow.server_client import post_event_sink

subscribe("flow_started", post_event_sink)
subscribe("flow_completed", post_event_sink)
subscribe("task_started", post_event_sink)
subscribe("task_ok", post_event_sink)
subscribe("task_err", post_event_sink)
```
`post_event_sink(payload)` reads `AUTO_WORKFLOW_SERVER_URL` and attaches `Authorization: Bearer <API_KEY>` when present. It supports batching, compression (gzip), and retry with backoff.

Performance & delivery:
- Use an asyncio queue + background task to batch/async POSTs to the server (HTTP keep-alive via httpx/aiohttp).
- Retry with backoff when server is temporarily unavailable; drop after N attempts (configurable), with optional disk spool for durability (P2).

Security (optional, later):
- API key or HMAC signature sent via header; server validates against configured key(s).
- TLS required in production; verify certificates.

---

## UI & SSR

- Templates (Jinja2):
  - `base.html`: Navbar, CSS, htmx/alpine hooks (optional)
  - `flows.html`: list known flows with counts and latest run status
  - `runs.html`: paginated recent runs
  - `run_detail.html`: flow/run metadata, tasks table, live badge, error summaries
- Real-time behavior:
  - When viewing `run_detail.html`, connect WebSocket `/ws/runs/{run_id}`.
  - On messages, update the tasks table row and run summary (status, counters) via small JS.
  - Logs panel: stream `task_log` events; filter by level/task.
- Styling: Minimal Tailwind CDN (or lightweight CSS) to keep things simple.

---

## Server Internals

- Persistence: Postgres as the source of truth (SQLAlchemy 2.0 ORM). In-memory caches for hot runs.
- Broadcaster: per-run subscriber lists; on update, publish JSON to connected WS. In multi-instance deployments, use Redis pub/sub (or Postgres LISTEN/NOTIFY) for fan-out.
- Pruning & retention: configurable retention policy with periodic cleanup (delete or archive to S3).

### Flow/Task Registration Strategy (Idempotent, Multi-Replica Safe)
- Define a stable `flow_key` on the client (e.g., `f"{module_path}:{flow_name}"`) and a stable `task_key` per task.
- On ingest of the first event for an unknown flow_key, attempt to upsert the Flow row:
  - Postgres `INSERT ... ON CONFLICT (flow_key) DO UPDATE SET updated_at = now()`.
  - This is safe across multiple replicas and avoids duplicate registration.
- Tasks are similarly upserted by `(flow_id, task_key)` with an ON CONFLICT clause.
- Cache flow_id/task_id in-memory per server instance for hot lookups; invalidate on schema changes or TTL expiration.
- Clients may optionally provide `flow_id` to skip lookup/registration when already known.

---

## Implementation Plan

P0 (Production-ready baseline):
- FastAPI app with:
  - POST `/api/v1/events` for ingest (supports `task_log`)
  - GET `/flows`, `/runs`, `/runs/{run_id}` SSR pages (Jinja2)
  - WS `/ws/runs/{run_id}` for live updates (flow/task/log)
- Postgres schema + Alembic migrations; DB URL from env (`DATABASE_URL`)
- Client: `server_client.py` with batching, API key header, env auto-bind logic
- Basic auth (API key) for ingest; rate limiting on `/api/v1/events`
- Dockerfile and Helm chart for Kubernetes deployment

P1 (Persistence & polish):
- Filters/search on the UI (by flow name, status, time range, tags)
- SSE fallback endpoint
- Server metrics (Prometheus-friendly counters and RED metrics)
- Flow/Task catalog registration endpoint + UI (descriptions, tags)
- Redis-based broadcast for multi-replica WS scaling

P2 (Advanced):
- Multi-process server (Uvicorn workers) with a pub/sub layer for WS broadcast (e.g., Redis)
- Project/tenant separation (namespaces)
- Historical charts (task durations, success rates)
- Durable client-side spool for offline telemetry; backpressure controls
- Attach bulk log ingestion, log search/indexing (e.g., Postgres full-text, optional OpenSearch)

---

## Detailed Implementation Plan

1) Bootstrap server package
- Create `server/` package with app factory, config, routers, templates, and static assets.
- Add optional `server` extra to pyproject for dependencies: fastapi, uvicorn, sqlalchemy>=2, alembic, jinja2, aiohttp or httpx, prometheus-client, slowapi (optional), redis (optional), psycopg[binary].

2) Configuration & logging
- `server/config.py`: Pydantic BaseSettings (DATABASE_URL, REDIS_URL, SERVER_API_KEY, LOG_LEVEL, CORS_ALLOWED_ORIGINS, RETENTION_DAYS).
- `server/logging.py`: structured logs, request ID middleware, access logs.

3) Database & migrations
- `server/db/base.py`, `server/db/session.py`: engine/session helpers.
- SQLAlchemy 2.0 ORM models (Declarative + mapper API) for Flow, Task, FlowRun, TaskRun, LogLine with indices and foreign keys.
- Alembic migrations in `server/db/migrations/`; Helm pre-upgrade hook runs `alembic upgrade head`.

4) Repositories and services
- Repositories: upsert flow/task, insert flow_run/task_run, append log_line, list/filter queries, pagination.
- Services: event normalization, status transitions, idempotency (optional `event_id`), broadcasting deltas to WS broker.

5) API & WebSocket
- `api/routes/ingest.py`: POST `/api/v1/events` with API key dependency.
- `api/routes/flows.py`, `runs.py`, `tasks.py`, `health.py` (live/readiness DB checks).
- `ws/endpoints.py`: `/ws/runs/{run_id}` broadcasts flow|task|log updates; in multi-replica, switch broker to Redis pub/sub.

6) UI (SSR)
- Jinja templates: `base.html`, `flows.html`, `runs.html`, `run_detail.html` with a WS-connected task table and logs panel.
- Minimal JS (or HTMX/Alpine) to patch DOM on incoming WS messages.

7) Metrics & rate limiting
- `/metrics` exposing Prometheus counters/histograms (requests, ingest rate, DB latency, WS clients).
- Optional rate limiting on `/api/v1/events` via slowapi (by API key/IP).

8) Client binding
- `auto_workflow/server_client.py`: async queue sender (aiohttp/httpx), API key header, gzip, backoff with jitter, batching, env auto-bind.
- Extend event subscriptions to include `task_log` events from the logging middleware (or explicit emit).

9) Docker & Helm
- Dockerfile multi-stage (builder → runtime), non-root user, healthcheck.
- Helm chart: Deployment, Service, Ingress, Secrets (API key, DB URL), ConfigMap, HPA, PDB, ServiceMonitor, migration Job/Hook.

10) Tests & CI/CD
- Unit/integration tests (httpx TestClient, ephemeral Postgres for DB tests, WS smoke test).
- CI: build image, run tests, alembic autogen check, vulnerability scan, stage deploy + migrations.

11) Ops runbook
- Backups/restores, migration safety, metrics/alerts (ingest failure %, WS clients, DB saturation), SLOs/SLIs.

---

## Package Structure (server/)

```
server/
  __init__.py
  app.py                 # FastAPI app factory
  config.py              # Pydantic settings
  logging.py             # Logging setup, request ID
  db/
    __init__.py
  base.py              # SQLAlchemy metadata/DeclarativeBase
    session.py           # engine/session helpers
    migrations/
      env.py
      versions/
  models/
    __init__.py
    flow.py              # Flow, Task models
    run.py               # FlowRun, TaskRun
    log.py               # LogLine
  schemas/
    __init__.py
    events.py            # Ingest DTOs
    flow.py              # API schemas
    run.py
    task.py
    log.py
  repositories/
    __init__.py
    flow_repo.py
    run_repo.py
    task_repo.py
    log_repo.py
  services/
    __init__.py
    ingest_service.py    # Apply events, call repos, broadcast
    run_service.py       # Queries/aggregations
    ws_broker.py         # In-memory/Redis broker
    auth.py              # API key dependency
    rate_limit.py        # Optional slowapi integration
  api/
    __init__.py
    deps.py              # Common deps (db, auth)
    routes/
      __init__.py
      ingest.py
      flows.py
      runs.py
      tasks.py
      health.py
  ws/
    __init__.py
    endpoints.py         # WebSocket endpoints
  web/
    __init__.py
    templates/
      base.html
      flows.html
      runs.html
      run_detail.html
    static/
      css/
      js/
  telemetry/
    __init__.py
    metrics.py           # Prometheus metrics
```

Top-level additions:
- `auto_workflow/server_client.py` (client sender and auto-binding)
- `Dockerfile`, `.dockerignore`
- `helm/auto-workflow-server/` chart (Deployment, Service, Ingress, migration Job/Hook)
- `docker-compose.yml` for local dev (server + Postgres + optional Redis)

### Required Env Vars
- `DATABASE_URL` (required), `SERVER_API_KEY` (required for ingest)
- `REDIS_URL` (optional), `LOG_LEVEL`, `CORS_ALLOWED_ORIGINS`, `RETENTION_DAYS`

### Local Dev Quickstart
```bash
docker compose up -d postgres
alembic upgrade head
uvicorn server.app:app --reload --port 8000
export AUTO_WORKFLOW_SERVER_URL=http://localhost:8000
export AUTO_WORKFLOW_SERVER_API_KEY=dev-key
python examples/data_pipeline.py
```
